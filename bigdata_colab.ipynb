{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bigdata_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPLD6wRVgcVaL7C4AAQN0FT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioNavarrete/bigdata-colab/blob/main/bigdata_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7BLNfIVJQ2M"
      },
      "source": [
        "# BigData Tools in Colab!\n",
        "\n",
        "The idea of this tutorial is provide you the classical BigData tools without using any of your own local resources.\n",
        "\n",
        "First, we will clone the repo that contains the needed scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTbLQcvWJhtX",
        "outputId": "bdf7d2e0-cfa8-4bbb-e37b-4904b4884eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!rm -r /content/bigdata-colab/\n",
        "!git clone https://github.com/MarioNavarrete/bigdata-colab.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/bigdata-colab/': No such file or directory\n",
            "Cloning into 'bigdata-colab'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 72 (delta 34), reused 54 (delta 20), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pICE2ITxJqVX"
      },
      "source": [
        "Preparing the dataset for this tutorial. All the scripts use a CSV file without index and header, so we will convert the parquet file into the needed formated CSV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_gXebZVJlO1",
        "outputId": "00b1734c-65f2-4623-cbb7-ec1319953bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "parquet_file = '/content/bigdata-colab/dataset/data.parquet'\n",
        "data = pd.read_parquet(parquet_file)\n",
        "data = data.dropna()\n",
        "data.reset_index(inplace = True, drop = True)\n",
        "data.to_csv('data.csv',index = False, header = False)\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-09-30 00:00:01</td>\n",
              "      <td>9.7</td>\n",
              "      <td>60.50</td>\n",
              "      <td>561.68</td>\n",
              "      <td>907.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-09-30 00:15:01</td>\n",
              "      <td>2.2</td>\n",
              "      <td>99.22</td>\n",
              "      <td>1818.84</td>\n",
              "      <td>1346.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-09-30 00:30:01</td>\n",
              "      <td>5.5</td>\n",
              "      <td>43.56</td>\n",
              "      <td>567.53</td>\n",
              "      <td>571.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-09-30 00:45:01</td>\n",
              "      <td>3.8</td>\n",
              "      <td>89.54</td>\n",
              "      <td>1821.72</td>\n",
              "      <td>933.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-09-30 01:00:01</td>\n",
              "      <td>6.4</td>\n",
              "      <td>107.69</td>\n",
              "      <td>1319.97</td>\n",
              "      <td>911.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             datetime    a       b        c       d\n",
              "0 2016-09-30 00:00:01  9.7   60.50   561.68   907.8\n",
              "1 2016-09-30 00:15:01  2.2   99.22  1818.84  1346.4\n",
              "2 2016-09-30 00:30:01  5.5   43.56   567.53   571.2\n",
              "3 2016-09-30 00:45:01  3.8   89.54  1821.72   933.8\n",
              "4 2016-09-30 01:00:01  6.4  107.69  1319.97   911.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6cStbseJ_RB"
      },
      "source": [
        "Instal the environment. This _executable.py_ will do all the work of installing and preparing the bigdata enviroment we need for using HDFS, MapReduce, Hive, Sqoop and Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MTPb5GHJ2NX",
        "outputId": "bee5407c-4aab-4ed9-e01b-ac821f74fdf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "exec(open('/content/bigdata-colab/executable.py').read())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Active services:\n",
            "1730 NameNode\n",
            "2210 Jps\n",
            "2167 JobHistoryServer\n",
            "1640 ResourceManager\n",
            "1998 DataNode\n",
            "2111 NodeManager\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqBNRVZTRYno"
      },
      "source": [
        "## HDFS and MapReduce\n",
        "\n",
        "Checking that we are able to use hadoop. We should expect that we have an empty file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsinoBFhzkXR"
      },
      "source": [
        "!hadoop fs -ls"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6C-70xJKtu8"
      },
      "source": [
        "Move the local file into our local HDFS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_9_7XPozw-p",
        "outputId": "c37ac66c-4c43-49b6-b7d8-52c52153c3a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!hadoop fs -put /content/data.csv data.csv\n",
        "!hadoop fs -ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup   56003136 2020-11-04 16:11 data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB-k98-Tz4Di"
      },
      "source": [
        "A single file as we expected.\n",
        "\n",
        "We already have created a very basic MapReduce job, and a bash script that run the needed commands to make it work. This should produce the min,max and the average of the second column, grouped by date.\n",
        "\n",
        "If you want, you can go directly and change the script to point to your own data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHo0i8GvKD9O",
        "outputId": "9edf6c34-171a-4cc3-aaf7-60e9379e685e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!bash /content/bigdata-colab/mapreduce/mapreduce.sh"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: `/user/root/Output_test': No such file or directory\n",
            "20/11/04 16:11:48 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/bigdata-colab/mapreduce/mapper.py, /content/bigdata-colab/mapreduce/reducer.py, /tmp/hadoop-unjar8916503154460475634/] [] /tmp/streamjob930844558876494403.jar tmpDir=null\n",
            "20/11/04 16:11:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
            "20/11/04 16:11:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
            "20/11/04 16:11:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
            "20/11/04 16:11:50 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "20/11/04 16:11:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1604506089806_0001\n",
            "20/11/04 16:11:51 INFO impl.YarnClientImpl: Submitted application application_1604506089806_0001\n",
            "20/11/04 16:11:51 INFO mapreduce.Job: The url to track the job: http://4bddbf5513f5:8088/proxy/application_1604506089806_0001/\n",
            "20/11/04 16:11:51 INFO mapreduce.Job: Running job: job_1604506089806_0001\n",
            "20/11/04 16:11:59 INFO mapreduce.Job: Job job_1604506089806_0001 running in uber mode : false\n",
            "20/11/04 16:11:59 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "20/11/04 16:12:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "20/11/04 16:12:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "20/11/04 16:12:20 INFO mapreduce.Job: Job job_1604506089806_0001 completed successfully\n",
            "20/11/04 16:12:20 INFO mapreduce.Job: Counters: 49\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=21548458\n",
            "\t\tFILE: Number of bytes written=43451380\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=56006104\n",
            "\t\tHDFS: Number of bytes written=20390\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=20375\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=5793\n",
            "\t\tTotal time spent by all map tasks (ms)=20375\n",
            "\t\tTotal time spent by all reduce tasks (ms)=5793\n",
            "\t\tTotal vcore-seconds taken by all map tasks=20375\n",
            "\t\tTotal vcore-seconds taken by all reduce tasks=5793\n",
            "\t\tTotal megabyte-seconds taken by all map tasks=20864000\n",
            "\t\tTotal megabyte-seconds taken by all reduce tasks=5932032\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=964416\n",
            "\t\tMap output records=964416\n",
            "\t\tMap output bytes=19619620\n",
            "\t\tMap output materialized bytes=21548464\n",
            "\t\tInput split bytes=184\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=546\n",
            "\t\tReduce shuffle bytes=21548464\n",
            "\t\tReduce input records=964416\n",
            "\t\tReduce output records=546\n",
            "\t\tSpilled Records=1928832\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=438\n",
            "\t\tCPU time spent (ms)=9280\n",
            "\t\tPhysical memory (bytes) snapshot=738738176\n",
            "\t\tVirtual memory (bytes) snapshot=5325381632\n",
            "\t\tTotal committed heap usage (bytes)=548405248\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=56005920\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=20390\n",
            "20/11/04 16:12:20 INFO streaming.StreamJob: Output directory: /user/root/Output_test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA5io0y1Qbgu"
      },
      "source": [
        "Check the outputs in HDFS and copy to local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0LpobegMCrQ",
        "outputId": "90811239-52c5-4a8f-9a05-6978be006e9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!hadoop fs -ls Out*"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2020-11-04 16:12 Output_test/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup      20390 2020-11-04 16:12 Output_test/part-00000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-mgIt55MLrl",
        "outputId": "c7c9d568-e824-44b7-d2be-58b04d4ff0de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir /content/mapreduce\n",
        "\n",
        "!hadoop fs -get /user/root/Output_test /content/mapreduce/output"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/11/04 16:12:25 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
            "20/11/04 16:12:25 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD5aqaheOUfX",
        "outputId": "4d65705b-88f7-4022-a2ce-0f570c4f9c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "columns = ['date','max_value','min_value','avg_value']\n",
        "\n",
        "output_mapred = pd.read_csv('/content/mapreduce/output/part-00000', sep = '\\t', names=columns)\n",
        "output_mapred.date = pd.to_datetime(output_mapred.date)\n",
        "output_mapred.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>max_value</th>\n",
              "      <th>min_value</th>\n",
              "      <th>avg_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-09-30</td>\n",
              "      <td>9317.58</td>\n",
              "      <td>0.1</td>\n",
              "      <td>397.034175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-10-01</td>\n",
              "      <td>10925.04</td>\n",
              "      <td>0.1</td>\n",
              "      <td>381.140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-10-02</td>\n",
              "      <td>12467.07</td>\n",
              "      <td>0.1</td>\n",
              "      <td>552.618267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-10-03</td>\n",
              "      <td>11381.92</td>\n",
              "      <td>0.1</td>\n",
              "      <td>402.385950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-10-04</td>\n",
              "      <td>13262.81</td>\n",
              "      <td>0.1</td>\n",
              "      <td>603.710668</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  max_value  min_value   avg_value\n",
              "0 2016-09-30    9317.58        0.1  397.034175\n",
              "1 2016-10-01   10925.04        0.1  381.140000\n",
              "2 2016-10-02   12467.07        0.1  552.618267\n",
              "3 2016-10-03   11381.92        0.1  402.385950\n",
              "4 2016-10-04   13262.81        0.1  603.710668"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrP3GI3A9KEH"
      },
      "source": [
        "## Hive\n",
        "\n",
        "For this small tutorial, we are going to create a new Hive table using 3 different methods:\n",
        "\n",
        "\n",
        "1.   Using an interactive Hive command line and pasting a HiveQL statement.\n",
        "2.   Running the same HiveQL statement from a .sql file.\n",
        "3.   Importing a MySQL table into Hive using Sqoop.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRLi056b-Fzv"
      },
      "source": [
        "#### Loading file from local path and running interactively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3Bs6Qrw93HB",
        "outputId": "37372a31-de45-4793-dd8c-38cf230ac3f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "allow_groupby = 'set hive.groupby.orderby.position.alias=true;'\n",
        "\n",
        "create_statement = \"\"\"\n",
        "create table test (datetime timestamp, a double, b double, c double, d double) row format delimited fields terminated by ',';\n",
        "\"\"\"\n",
        "\n",
        "load_data = \"LOAD DATA LOCAL INPATH '/content/data.csv' OVERWRITE INTO TABLE test;\"\n",
        "\n",
        "sql = \"\"\"\n",
        "select to_date(datetime),\n",
        "avg(a) avg_a,\n",
        "avg(b) avg_b,\n",
        "avg(c) avg_c,\n",
        "avg(d) avg_d,\n",
        "min(a) min_a,\n",
        "min(b) min_b,\n",
        "min(c) min_c,\n",
        "min(d) min_d,\n",
        "max(a) max_a,\n",
        "max(b) max_b,\n",
        "max(c) max_c,\n",
        "max(d) max_d\n",
        "from test\n",
        "group by 1\n",
        "limit 10;\n",
        "\"\"\"\n",
        "\n",
        "exit = \"exit;\"\n",
        "\n",
        "full_statement = ''.join([allow_groupby,create_statement,load_data,sql,exit])\n",
        "full_statement"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"set hive.groupby.orderby.position.alias=true;\\ncreate table test (datetime timestamp, a double, b double, c double, d double) row format delimited fields terminated by ',';\\nLOAD DATA LOCAL INPATH '/content/data.csv' OVERWRITE INTO TABLE test;\\nselect to_date(datetime),\\navg(a) avg_a,\\navg(b) avg_b,\\navg(c) avg_c,\\navg(d) avg_d,\\nmin(a) min_a,\\nmin(b) min_b,\\nmin(c) min_c,\\nmin(d) min_d,\\nmax(a) max_a,\\nmax(b) max_b,\\nmax(c) max_c,\\nmax(d) max_d\\nfrom test\\ngroup by 1\\nlimit 10;\\nexit;\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKNdTEMOOivj",
        "outputId": "f31cdb16-0a8d-4d67-e137-3a4237e5ac06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##you need to copy the printed statement and pasting into the command line that \n",
        "##will appear in this block once you run it\n",
        "!hive"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/content/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/content/hadoop-2.7.0/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
            "\n",
            "Logging initialized using configuration in jar:file:/content/apache-hive-2.3.7-bin/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: true\n",
            "Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
            "hive> set hive.groupby.orderby.position.alias=true; create table test (datetime timestamp, a double, b double, c double, d double) row format delimited fields terminated by ','; LOAD DATA LOCAL INPATH '/content/data.csv' OVERWRITE INTO TABLE test; select to_date(datetime), avg(a) avg_a, avg(b) avg_b, avg(c) avg_c, avg(d) avg_d, min(a) min_a, min(b) min_b, min(c) min_c, min(d) min_d, max(a) max_a, max(b) max_b, max(c) max_c, max(d) max_d from test group by 1 limit 10; exit;\n",
            "OK\n",
            "Time taken: 7.846 seconds\n",
            "Loading data to table default.test\n",
            "OK\n",
            "Time taken: 2.342 seconds\n",
            "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
            "Query ID = root_20201104161336_fbb855c4-95e4-48d1-8541-774ee9cfaa8b\n",
            "Total jobs = 1\n",
            "Launching Job 1 out of 1\n",
            "Number of reduce tasks not specified. Estimated from input data size: 1\n",
            "In order to change the average load for a reducer (in bytes):\n",
            "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
            "In order to limit the maximum number of reducers:\n",
            "  set hive.exec.reducers.max=<number>\n",
            "In order to set a constant number of reducers:\n",
            "  set mapreduce.job.reduces=<number>\n",
            "Starting Job = job_1604506089806_0002, Tracking URL = http://4bddbf5513f5:8088/proxy/application_1604506089806_0002/\n",
            "Kill Command = /content/hadoop/bin/hadoop job  -kill job_1604506089806_0002\n",
            "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
            "2020-11-04 16:13:46,178 Stage-1 map = 0%,  reduce = 0%\n",
            "2020-11-04 16:13:58,351 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 10.04 sec\n",
            "2020-11-04 16:14:04,763 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.99 sec\n",
            "MapReduce Total cumulative CPU time: 11 seconds 990 msec\n",
            "Ended Job = job_1604506089806_0002\n",
            "MapReduce Jobs Launched: \n",
            "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 11.99 sec   HDFS Read: 56018483 HDFS Write: 1630 SUCCESS\n",
            "Total MapReduce CPU Time Spent: 11 seconds 990 msec\n",
            "OK\n",
            "_c0\tavg_a\tavg_b\tavg_c\tavg_d\tmin_a\tmin_b\tmin_c\tmin_d\tmax_a\tmax_b\tmax_c\tmax_d\n",
            "2016-09-30\t396.2390833333333\t70.59916666666668\t803.904291666667\t1617.6736875\t0.1\t1.1\t6.25\t13.07\t9317.58\t222.75\t3001.68\t4406.49\n",
            "2016-10-01\t380.35095833333344\t67.8066041666667\t831.0852708333329\t1830.497854166665\t0.1\t0.54\t8.81\t18.28\t10925.04\t222.75\t3196.7599999999998\t4666.74\n",
            "2016-10-02\t551.4694791666666\t67.49656250000005\t956.866541666667\t1795.0325208333338\t0.1\t1.05\t9.06\t21.14\t12467.070000000002\t222.75\t3786.72\t4272.96\n",
            "2016-10-03\t401.5628541666667\t67.35631249999994\t856.046791666667\t1857.349208333334\t0.1\t0.54\t17.07\t22.18\t11381.92\t218.25\t2883.79\t4339.17\n",
            "2016-10-04\t602.4543958333337\t62.76006249999997\t1022.9292291666659\t1790.7977708333326\t0.1\t0.53\t10.96\t24.0\t13262.81\t222.75\t3414.2000000000003\t4344.12\n",
            "2016-10-05\t526.0749999999999\t66.08541666666666\t977.347562500001\t1804.760041666666\t0.1\t1.04\t18.59\t28.59\t10796.72\t222.75\t3533.0499999999997\t4244.38\n",
            "2016-10-06\t503.64766666666713\t70.08229166666665\t1009.1162291666662\t1786.8192083333317\t0.1\t0.53\t11.99\t29.42\t10664.7\t213.84\t3707.3399999999997\t4270.86\n",
            "2016-10-07\t525.4132083333335\t67.80087500000005\t1003.0338541666665\t1684.7339374999997\t0.1\t0.53\t18.19\t31.15\t12600.72\t213.84\t3407.61\t4258.9800000000005\n",
            "2016-10-08\t495.9749375\t63.471333333333355\t950.5857499999998\t1744.5025\t0.2\t0.54\t8.57\t39.02\t12615.82\t213.84\t3321.7999999999997\t4754.97\n",
            "2016-10-09\t536.7492499999995\t63.49022916666665\t948.547499999999\t1792.9299791666667\t0.1\t0.54\t10.02\t28.06\t11067.93\t213.84\t3595.41\t4599.14\n",
            "Time taken: 29.42 seconds, Fetched: 10 row(s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLRJhnuN_bim"
      },
      "source": [
        "#### Running the same steps using a .sql file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fDgHFNu92KG"
      },
      "source": [
        "!hive -f /content/bigdata-colab/hive/script.sql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTY0AFh4BwUF"
      },
      "source": [
        "#### MySQL + Sqoop to Hive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZkSxuPLCuhP"
      },
      "source": [
        "First we need to load our data into our MySQL data base, and then grab all that data and move it into Hive. We will create the same table we created in the past blocks, but we are going to name it _test3_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgSNcf2E_X1N",
        "outputId": "de0ffec1-b850-4dc0-cadc-52256d6846b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mysql -u root --password=password testdb < /content/bigdata-colab/mysql/load_data.sql"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mysql: [Warning] Using a password on the command line interface can be insecure.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1-o6cUjCIGl",
        "outputId": "15421cec-4039-498e-c2d4-db28dc29f50d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mysql -u root --password=password testdb -e \"select * from test3 limit 5\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mysql: [Warning] Using a password on the command line interface can be insecure.\n",
            "+---------------------+------+--------+---------+--------+\n",
            "| datetime            | a    | b      | c       | d      |\n",
            "+---------------------+------+--------+---------+--------+\n",
            "| 2016-09-30 00:00:01 |  9.7 |   60.5 |  561.68 |  907.8 |\n",
            "| 2016-09-30 00:15:01 |  2.2 |  99.22 | 1818.84 | 1346.4 |\n",
            "| 2016-09-30 00:30:01 |  5.5 |  43.56 |  567.53 |  571.2 |\n",
            "| 2016-09-30 00:45:01 |  3.8 |  89.54 | 1821.72 |  933.8 |\n",
            "| 2016-09-30 01:00:01 |  6.4 | 107.69 | 1319.97 |  911.2 |\n",
            "+---------------------+------+--------+---------+--------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE9-jCyHCQqf"
      },
      "source": [
        "!sqoop import --connect jdbc:mysql://localhost/testdb --username root --password password --table test3 --hive-import -m 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtSbWxAs6Ilp"
      },
      "source": [
        "Same as the first Hive block, we will need to copy the sql statement and printed into the command line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O77utcYDBkZ",
        "outputId": "f1776403-d177-492c-b0b2-b1ca26a4e9b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sql3 = \"\"\"set hive.groupby.orderby.position.alias=true;\n",
        "select to_date(datetime),\n",
        "avg(a) avg_a,\n",
        "avg(b) avg_b,\n",
        "avg(c) avg_c,\n",
        "avg(d) avg_d,\n",
        "min(a) min_a,\n",
        "min(b) min_b,\n",
        "min(c) min_c,\n",
        "min(d) min_d,\n",
        "max(a) max_a,\n",
        "max(b) max_b,\n",
        "max(c) max_c,\n",
        "max(d) max_d\n",
        "from test3\n",
        "group by 1\n",
        "limit 10;\n",
        "exit;\n",
        "\"\"\"\n",
        "!hive"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/content/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/content/hadoop-2.7.0/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
            "\n",
            "Logging initialized using configuration in jar:file:/content/apache-hive-2.3.7-bin/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: true\n",
            "Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
            "hive> set hive.groupby.orderby.position.alias=true; select to_date(datetime), avg(a) avg_a, avg(b) avg_b, avg(c) avg_c, avg(d) avg_d, min(a) min_a, min(b) min_b, min(c) min_c, min(d) min_d, max(a) max_a, max(b) max_b, max(c) max_c, max(d) max_d from test3 group by 1 limit 10; exit;\n",
            "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
            "Query ID = root_20201104161701_2e7e4a29-365f-48ed-8e64-7fc17ca6bad8\n",
            "Total jobs = 1\n",
            "Launching Job 1 out of 1\n",
            "Number of reduce tasks not specified. Estimated from input data size: 1\n",
            "In order to change the average load for a reducer (in bytes):\n",
            "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
            "In order to limit the maximum number of reducers:\n",
            "  set hive.exec.reducers.max=<number>\n",
            "In order to set a constant number of reducers:\n",
            "  set mapreduce.job.reduces=<number>\n",
            "Starting Job = job_1604506089806_0005, Tracking URL = http://4bddbf5513f5:8088/proxy/application_1604506089806_0005/\n",
            "Kill Command = /content/hadoop/bin/hadoop job  -kill job_1604506089806_0005\n",
            "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
            "2020-11-04 16:17:17,324 Stage-1 map = 0%,  reduce = 0%\n",
            "2020-11-04 16:17:29,374 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 10.08 sec\n",
            "2020-11-04 16:17:35,797 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.98 sec\n",
            "MapReduce Total cumulative CPU time: 11 seconds 980 msec\n",
            "Ended Job = job_1604506089806_0005\n",
            "MapReduce Jobs Launched: \n",
            "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 11.98 sec   HDFS Read: 45596881 HDFS Write: 1552 SUCCESS\n",
            "Total MapReduce CPU Time Spent: 11 seconds 980 msec\n",
            "OK\n",
            "_c0\tavg_a\tavg_b\tavg_c\tavg_d\tmin_a\tmin_b\tmin_c\tmin_d\tmax_a\tmax_b\tmax_c\tmax_d\n",
            "2016-09-30\t396.23908333333327\t70.59916666666668\t803.904291666667\t1617.6736875\t0.1\t1.1\t6.25\t13.07\t9317.58\t222.75\t3001.68\t4406.49\n",
            "2016-10-01\t380.35087500000003\t67.8066041666667\t831.0852708333329\t1830.497854166665\t0.1\t0.54\t8.81\t18.28\t10925.0\t222.75\t3196.76\t4666.74\n",
            "2016-10-02\t551.4695416666667\t67.49656250000005\t956.866541666667\t1795.0325208333338\t0.1\t1.05\t9.06\t21.14\t12467.1\t222.75\t3786.72\t4272.96\n",
            "2016-10-03\t401.56281250000006\t67.35631249999994\t856.046791666667\t1857.349208333334\t0.1\t0.54\t17.07\t22.18\t11381.9\t218.25\t2883.79\t4339.17\n",
            "2016-10-04\t602.4544375000003\t62.76006249999997\t1022.9292291666659\t1790.7977708333326\t0.1\t0.53\t10.96\t24.0\t13262.8\t222.75\t3414.2\t4344.12\n",
            "2016-10-05\t526.0749166666666\t66.08541666666666\t977.347562500001\t1804.7600416666662\t0.1\t1.04\t18.59\t28.59\t10796.7\t222.75\t3533.05\t4244.38\n",
            "2016-10-06\t503.64766666666713\t70.08229166666665\t1009.1162291666662\t1786.819208333332\t0.1\t0.53\t11.99\t29.42\t10664.7\t213.84\t3707.34\t4270.86\n",
            "2016-10-07\t525.4130833333335\t67.80087500000005\t1003.0338541666665\t1684.7339374999997\t0.1\t0.53\t18.19\t31.15\t12600.7\t213.84\t3407.61\t4258.98\n",
            "2016-10-08\t495.97497916666657\t63.471333333333355\t950.5857499999998\t1744.5025\t0.2\t0.54\t8.57\t39.02\t12615.8\t213.84\t3321.8\t4754.97\n",
            "2016-10-09\t536.7491458333328\t63.49022916666665\t948.547499999999\t1792.9299791666667\t0.1\t0.54\t10.02\t28.06\t11067.9\t213.84\t3595.41\t4599.14\n",
            "Time taken: 36.649 seconds, Fetched: 10 row(s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Prys5gOEflK"
      },
      "source": [
        "## Spark\n",
        "\n",
        "For this short tutorial, we are going to load the dataset directly from Hive, and also, directly from the parquet and csv files that will be stored in HDFS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6YZVs2TDehT"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.enableHiveSupport().appName(\"bigdata-tools\").getOrCreate()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKyZDXIOE2yF"
      },
      "source": [
        "#### Reading from Hive\n",
        "\n",
        "We are going to re-use the sql statement we created in the first Hive block. We dont need to run the create statements due we enable HiveSupport in our Spark session (using _test3_ table this time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0P9I182EiWX",
        "outputId": "1d8cf8d1-a1e6-4c74-eace-66bcc29f40d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sql = \"\"\"\n",
        "select to_date(datetime),\n",
        "avg(a) avg_a,\n",
        "avg(b) avg_b,\n",
        "avg(c) avg_c,\n",
        "avg(d) avg_d,\n",
        "min(a) min_a,\n",
        "min(b) min_b,\n",
        "min(c) min_c,\n",
        "min(d) min_d,\n",
        "max(a) max_a,\n",
        "max(b) max_b,\n",
        "max(c) max_c,\n",
        "max(d) max_d\n",
        "from test3\n",
        "group by 1\n",
        "order by 1 asc;\n",
        "\"\"\"\n",
        "spark.sql(sql[:-2]).show(10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------+------------------+------------------+------------------+------------------+-----+-----+-----+-----+-------+------+-------+-------+\n",
            "|to_date(default.test3.`datetime`)|             avg_a|             avg_b|             avg_c|             avg_d|min_a|min_b|min_c|min_d|  max_a| max_b|  max_c|  max_d|\n",
            "+---------------------------------+------------------+------------------+------------------+------------------+-----+-----+-----+-----+-------+------+-------+-------+\n",
            "|                       2016-09-30|396.23908333333327| 70.59916666666668|  803.904291666667|      1617.6736875|  0.1|  1.1| 6.25|13.07|9317.58|222.75|3001.68|4406.49|\n",
            "|                       2016-10-01|380.35087500000003|  67.8066041666667| 831.0852708333329| 1830.497854166665|  0.1| 0.54| 8.81|18.28|10925.0|222.75|3196.76|4666.74|\n",
            "|                       2016-10-02| 551.4695416666667| 67.49656250000005|  956.866541666667|1795.0325208333338|  0.1| 1.05| 9.06|21.14|12467.1|222.75|3786.72|4272.96|\n",
            "|                       2016-10-03|401.56281250000006| 67.35631249999994|  856.046791666667| 1857.349208333334|  0.1| 0.54|17.07|22.18|11381.9|218.25|2883.79|4339.17|\n",
            "|                       2016-10-04| 602.4544375000003| 62.76006249999997|1022.9292291666659|1790.7977708333326|  0.1| 0.53|10.96| 24.0|13262.8|222.75| 3414.2|4344.12|\n",
            "|                       2016-10-05| 526.0749166666666| 66.08541666666666|  977.347562500001|1804.7600416666662|  0.1| 1.04|18.59|28.59|10796.7|222.75|3533.05|4244.38|\n",
            "|                       2016-10-06|503.64766666666713| 70.08229166666665|1009.1162291666662| 1786.819208333332|  0.1| 0.53|11.99|29.42|10664.7|213.84|3707.34|4270.86|\n",
            "|                       2016-10-07| 525.4130833333335| 67.80087500000005|1003.0338541666665|1684.7339374999997|  0.1| 0.53|18.19|31.15|12600.7|213.84|3407.61|4258.98|\n",
            "|                       2016-10-08|495.97497916666657|63.471333333333355| 950.5857499999998|         1744.5025|  0.2| 0.54| 8.57|39.02|12615.8|213.84| 3321.8|4754.97|\n",
            "|                       2016-10-09| 536.7491458333328| 63.49022916666665|  948.547499999999|1792.9299791666667|  0.1| 0.54|10.02|28.06|11067.9|213.84|3595.41|4599.14|\n",
            "+---------------------------------+------------------+------------------+------------------+------------------+-----+-----+-----+-----+-------+------+-------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3eNqzBTE7Oj"
      },
      "source": [
        "#### Reading parquet file\n",
        "\n",
        "We need to put the file into HDFS, and then we can read it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTEb2_nBE5i8",
        "outputId": "e94a50cd-ca7b-4828-cdd1-4c1f13d9b6b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!hadoop fs -put /content/bigdata-colab/dataset/data.parquet data.parquet\n",
        "\n",
        "spark_df = spark.read.parquet('/user/root/data.parquet')\n",
        "spark_df.show(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+------------------+-----------------+------------------+------------------+\n",
            "|           datetime|                 a|                b|                 c|                 d|\n",
            "+-------------------+------------------+-----------------+------------------+------------------+\n",
            "|2016-09-30 00:00:01| 9.700000000000001|             60.5|            561.68|             907.8|\n",
            "|2016-09-30 00:15:01|               2.2|            99.22|           1818.84|1346.3999999999999|\n",
            "|2016-09-30 00:30:01|               5.5|            43.56|            567.53| 571.1999999999999|\n",
            "|2016-09-30 00:45:01|3.8000000000000003|89.53999999999999|1821.7199999999998|             933.8|\n",
            "|2016-09-30 01:00:01|               6.4|           107.69|           1319.97| 911.1999999999999|\n",
            "+-------------------+------------------+-----------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5b0BdI79V3"
      },
      "source": [
        "We will create the last table _test4_ with the same dataset, and then we will use spark sql to run the spark sql statement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt-LOqXnFLLX"
      },
      "source": [
        "spark_df.createOrReplaceTempView(\"test4\")\n",
        "sql = \"\"\"\n",
        "select to_date(datetime),\n",
        "avg(a) avg_a,\n",
        "avg(b) avg_b,\n",
        "avg(c) avg_c,\n",
        "avg(d) avg_d,\n",
        "min(a) min_a,\n",
        "min(b) min_b,\n",
        "min(c) min_c,\n",
        "min(d) min_d,\n",
        "max(a) max_a,\n",
        "max(b) max_b,\n",
        "max(c) max_c,\n",
        "max(d) max_d\n",
        "from test4\n",
        "group by 1\n",
        "order by 1 asc;\n",
        "\"\"\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPpNe8d3HjUW",
        "outputId": "17a4c083-a7c2-4e7d-bc5f-de36cad82ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.sql(sql[:-2]).show(5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------+------------------+-----------------+------------------+------------------+-----+-----+-----+-----+------------------+------+------------------+-------+\n",
            "|to_date(test4.`datetime`)|             avg_a|            avg_b|             avg_c|             avg_d|min_a|min_b|min_c|min_d|             max_a| max_b|             max_c|  max_d|\n",
            "+-------------------------+------------------+-----------------+------------------+------------------+-----+-----+-----+-----+------------------+------+------------------+-------+\n",
            "|               2016-09-30| 396.2390833333333|70.59916666666668|  803.904291666667|      1617.6736875|  0.1|  1.1| 6.25|13.07|           9317.58|222.75|           3001.68|4406.49|\n",
            "|               2016-10-01|380.35095833333344| 67.8066041666667| 831.0852708333329| 1830.497854166665|  0.1| 0.54| 8.81|18.28|          10925.04|222.75|3196.7599999999998|4666.74|\n",
            "|               2016-10-02| 551.4694791666666|67.49656250000005|  956.866541666667|1795.0325208333338|  0.1| 1.05| 9.06|21.14|12467.070000000002|222.75|           3786.72|4272.96|\n",
            "|               2016-10-03| 401.5628541666667|67.35631249999994|  856.046791666667| 1857.349208333334|  0.1| 0.54|17.07|22.18|          11381.92|218.25|           2883.79|4339.17|\n",
            "|               2016-10-04| 602.4543958333337|62.76006249999997|1022.9292291666659|1790.7977708333326|  0.1| 0.53|10.96| 24.0|          13262.81|222.75|3414.2000000000003|4344.12|\n",
            "+-------------------------+------------------+-----------------+------------------+------------------+-----+-----+-----+-----+------------------+------+------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8NVcqCT-riM"
      },
      "source": [
        "#### Reading csv file\n",
        "\n",
        "We are going to use the CSV we already stored on HDFS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n30hyqhHl94",
        "outputId": "09c953d4-b930-406b-806f-b952c0880c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark_df_csv = spark.read.csv('/user/root/data.csv', \n",
        "                          schema = 'datetime timestamp, a float, b float, c float, d float',\n",
        "                          sep = ',')\n",
        "spark_df_csv.show(5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+---+------+-------+------+\n",
            "|           datetime|  a|     b|      c|     d|\n",
            "+-------------------+---+------+-------+------+\n",
            "|2016-09-30 00:00:01|9.7|  60.5| 561.68| 907.8|\n",
            "|2016-09-30 00:15:01|2.2| 99.22|1818.84|1346.4|\n",
            "|2016-09-30 00:30:01|5.5| 43.56| 567.53| 571.2|\n",
            "|2016-09-30 00:45:01|3.8| 89.54|1821.72| 933.8|\n",
            "|2016-09-30 01:00:01|6.4|107.69|1319.97| 911.2|\n",
            "+-------------------+---+------+-------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzqThL_S9sZf",
        "outputId": "cda813d0-1095-4df7-aa7e-91cf88bf5844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark_df_csv.createOrReplaceTempView(\"test5\")\n",
        "sql = \"\"\"\n",
        "select to_date(datetime),\n",
        "avg(a) avg_a,\n",
        "avg(b) avg_b,\n",
        "avg(c) avg_c,\n",
        "avg(d) avg_d,\n",
        "min(a) min_a,\n",
        "min(b) min_b,\n",
        "min(c) min_c,\n",
        "min(d) min_d,\n",
        "max(a) max_a,\n",
        "max(b) max_b,\n",
        "max(c) max_c,\n",
        "max(d) max_d\n",
        "from test5\n",
        "group by 1\n",
        "order by 1 asc;\n",
        "\"\"\"\n",
        "spark.sql(sql[:-2]).show(5)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------+------------------+-----------------+------------------+------------------+-----+-----+-----+-----+--------+------+-------+-------+\n",
            "|to_date(test5.`datetime`)|             avg_a|            avg_b|             avg_c|             avg_d|min_a|min_b|min_c|min_d|   max_a| max_b|  max_c|  max_d|\n",
            "+-------------------------+------------------+-----------------+------------------+------------------+-----+-----+-----+-----+--------+------+-------+-------+\n",
            "|               2016-09-30| 396.2390839710211|70.59916661133369| 803.9042923967044|1617.6736891508103|  0.1|  1.1| 6.25|13.07| 9317.58|222.75|3001.68|4406.49|\n",
            "|               2016-10-01|380.35096037131734|67.80660413354636| 831.0852713167667| 1830.497857928276|  0.1| 0.54| 8.81|18.28|10925.04|222.75|3196.76|4666.74|\n",
            "|               2016-10-02| 551.4694790929556|67.49656240890424| 956.8665415167809|1795.0325181643168|  0.1| 1.05| 9.06|21.14|12467.07|222.75|3786.72|4272.96|\n",
            "|               2016-10-03| 401.5628545624204|67.35631241612137|  856.046790210406|  1857.34920779864|  0.1| 0.54|17.07|22.18|11381.92|218.25|2883.79|4339.17|\n",
            "|               2016-10-04| 602.4543972864902|62.76006246929367|1022.9292270044485|1790.7977761824925|  0.1| 0.53|10.96| 24.0|13262.81|222.75| 3414.2|4344.12|\n",
            "+-------------------------+------------------+-----------------+------------------+------------------+-----+-----+-----+-----+--------+------+-------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KqRPsruEAuD"
      },
      "source": [
        "There are some small differences in the outputs due the different precision type we are using for our data.\n"
      ]
    }
  ]
}